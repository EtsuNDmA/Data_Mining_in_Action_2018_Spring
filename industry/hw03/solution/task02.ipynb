{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = make_classification(\n",
    "    n_samples=10000, n_features=20, \n",
    "    n_classes=2, n_informative=20, \n",
    "    n_redundant=0,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression(BaseEstimator):\n",
    "    def __init__(self, C=1.0, random_state=None, iters=100,\n",
    "                 batch_size=1000, step=0.1, penalty='l2'\n",
    "                ):\n",
    "        self.C = C\n",
    "        self.random_state = random_state\n",
    "        self.iters = iters\n",
    "        self.batch_size = batch_size\n",
    "        self.step = step\n",
    "        self.penalty = penalty\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1.0 / (1 + np.exp(-z))\n",
    "    \n",
    "    # будем пользоваться этой функцией для подсчёта <w, x>\n",
    "    def __predict(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.w) + self.w0)\n",
    "\n",
    "    # sklearn нужно, чтобы predict возвращал классы, поэтому оборачиваем наш __predict в это\n",
    "    def predict(self, X):\n",
    "        res = self.__predict(X)\n",
    "        res[res >= 0.5] = 1\n",
    "        res[res < 0.5] = 0\n",
    "        return res\n",
    "    \n",
    "    # производная регуляризатора\n",
    "    def der_reg(self):\n",
    "        if self.penalty == 'l1':\n",
    "            return self.C * np.sign(self.w)\n",
    "        elif self.penalty == 'l2':\n",
    "            return self.C *self.w\n",
    "\n",
    "    # будем считать стохастический градиент не на одном элементе, а сразу на пачке (чтобы было эффективнее)\n",
    "    def der_loss(self, x, y):\n",
    "        # x.shape == (batch_size, features)\n",
    "        # y.shape == (batch_size,)\n",
    "\n",
    "        # считаем производную по каждой координате на каждом объекте\n",
    "        # для масштаба возвращаем средний градиент по пачке\n",
    "        d = self.__predict(x) - y\n",
    "        ders_w = np.mean(x * d[:, np.newaxis], axis=0)\n",
    "        der_w0 = np.mean(d)\n",
    "        return ders_w, der_w0\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # RandomState для воспроизводитмости\n",
    "        random_gen = np.random.RandomState(self.random_state)\n",
    "        \n",
    "        # получаем размерности матрицы\n",
    "        size, dim = X_train.shape\n",
    "        \n",
    "        # случайная начальная инициализация\n",
    "        self.w = random_gen.rand(dim)\n",
    "        self.w0 = random_gen.randn()\n",
    "        self.learning_curve = np.zeros(self.iters)\n",
    "        for i in range(self.iters):  \n",
    "            # берём случайный набор элементов\n",
    "            rand_indices = random_gen.choice(size, self.batch_size)\n",
    "            # исходные метки классов это 0/1\n",
    "            X = X_train[rand_indices]\n",
    "            y = y_train[rand_indices]\n",
    "            self.learning_curve[i] += log_loss(y,self.__predict(X), labels=(0,1))\n",
    "            # считаем производные\n",
    "            der_w, der_w0 = self.der_loss(X, y)\n",
    "            der_w += self.der_reg()\n",
    "            # обновляемся по антиградиенту\n",
    "            self.w -= der_w * self.step\n",
    "            self.w0 -= der_w0 * self.step\n",
    "        # метод fit для sklearn должен возвращать self\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss='log', learning_rate='constant', eta0=0.1, max_iter=1000)\n",
    "\n",
    "my_clf = MyLogisticRegression(penalty='l2', step=0.01, batch_size=100, iters=1000, C=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7125\n",
      "0.7895\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(cross_val_score(clf, X_data, y_data, cv=2, scoring='accuracy')))\n",
    "print(np.mean(cross_val_score(my_clf, X_data, y_data, cv=2, scoring='accuracy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1152 candidates, totalling 2304 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 222 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 472 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 822 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1272 tasks      | elapsed: 25.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1822 tasks      | elapsed: 38.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2304 out of 2304 | elapsed: 51.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 s, sys: 757 ms, total: 12.2 s\n",
      "Wall time: 51min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {\n",
    "    'C': [2,1,0.1],\n",
    "    'random_state': [1, 25, 42],\n",
    "    'iters': [100,1000,5000],\n",
    "    'batch_size': [10,100,1000],\n",
    "    'step': [0.01,0.1,0.5],\n",
    "    'penalty': ['l1','l2'],\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'C': [2,1,0.5,0.1],\n",
    "    'random_state': [1, 25, 42],\n",
    "    'iters': [100,1000,5000,10000],\n",
    "    'batch_size': [100,500,1000],\n",
    "    'step': [0.005,0.01,0.1,0.5],\n",
    "    'penalty': ['l1','l2'],\n",
    "}\n",
    "\n",
    "grid_pipe = GridSearchCV(my_clf, param_grid, \n",
    "                         scoring='accuracy',\n",
    "                         n_jobs=-1, cv=2,\n",
    "                         verbose=1).fit(X_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7995,\n",
       " {'C': 0.1,\n",
       "  'batch_size': 100,\n",
       "  'iters': 5000,\n",
       "  'penalty': 'l2',\n",
       "  'random_state': 25,\n",
       "  'step': 0.1})"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_pipe.best_score_, grid_pipe.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
